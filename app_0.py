import streamlit as st
import os
import glob
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings

# ==========================================
# ‚öôÔ∏è Configuration
# ==========================================
os.environ["GOOGLE_API_KEY"] = "Api key................" 

# ==========================================
# 1. RAG Backend Logic
# ==========================================
@st.cache_resource
def init_knowledge_base(data_folder="data"):
    files = glob.glob(f"{data_folder}/*.txt") + glob.glob(f"{data_folder}/*.pdf")
    
    if not files:
        st.error(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {data_folder}")
        return None

    documents = []
    for file in files:
        try:
            if file.endswith('.pdf'): 
                loader = PyPDFLoader(file)
            else:
                loader = TextLoader(file, encoding='utf-8')
            documents.extend(loader.load())
        except Exception as e:
            st.error(f"‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
            
    if not documents:
        return None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
    
    with st.spinner("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢..."):
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        vector_db = FAISS.from_documents(chunks, embeddings)
    
    return vector_db

def ask_rag_bot(query, vector_db):
    prompt_template = """
    ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Ç‡∏≠‡∏á Finansia (Finansia AI Assistant)
    ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Context ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    
    ‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:
    1. ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô Context ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ ‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"
    2. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏≠‡∏á‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
    3. ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå

    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (Context): {context}
    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (Question): {question}
    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0.3,
            transport="rest"
        )

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )

        response = qa_chain.invoke({"query": query})
        return response

    except Exception as e:
        return {"result": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}", "source_documents": []}

# ==========================================
# 2. Streamlit UI (Frontend)
# ==========================================
def main():
    st.set_page_config(page_title="Finansia AI Assistant", layout="wide")
    st.title("üí∞ Finansia AI Assistant (RAG Chatbot)")

    if "vector_db" not in st.session_state:
        db = init_knowledge_base("data")
        if db:
            st.session_state.vector_db = db
            st.toast("‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!", icon="‚úÖ")

    if "vector_db" in st.session_state:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        query = st.chat_input("‚ùì ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô...")

        if query:
            st.chat_message("user").write(query)
            st.session_state.messages.append({"role": "user", "content": query})

            with st.chat_message("assistant"):
                with st.spinner("ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
                    response = ask_rag_bot(query, st.session_state.vector_db)
                    answer_text = response["result"]
                    
                    unique_sources = {os.path.basename(doc.metadata['source']) for doc in response.get("source_documents", [])}
                    
                    st.write(answer_text)
                    
                    final_msg = answer_text
                    if unique_sources:
                        st.markdown("---")
                        st.caption("üìö **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**")
                        source_text = "\n\nüìö **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**"
                        for src in unique_sources:
                            st.caption(f"- üìÑ {src}")
                            source_text += f"\n- üìÑ {src}"
                        final_msg += source_text

            st.session_state.messages.append({"role": "assistant", "content": final_msg})

if __name__ == "__main__":
    main()